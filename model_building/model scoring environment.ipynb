{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import metrics, tree, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold,cross_val_score,train_test_split,LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from statistics import mean\n",
    "\n",
    "from data_load import *\n",
    "from dictionaries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the model experiment names and all of the used suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep the suffixes and the experiment model names updated to view all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model_names = [\n",
    "#                         'd0_b0_c0_v0',\n",
    "                        'd0_b0_c1_v0'\n",
    "                        ]\n",
    "\n",
    "# It takes about 10 mins to score each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [\n",
    "                'problem_type', \n",
    "                'creative', \n",
    "                'outdoors', \n",
    "                'career',\n",
    "                'group_work', \n",
    "                'liked_courses', \n",
    "                'disliked_courses', \n",
    "                'programming',\n",
    "                'join_clubs', \n",
    "                'not_clubs', \n",
    "                'liked_projects',\n",
    "                'disliked_projects',\n",
    "                'tv_shows', \n",
    "                'alternate_degree', \n",
    "                'expensive_equipment', \n",
    "                'drawing',\n",
    "                'essay', \n",
    "                'architecture', \n",
    "                'automotive', \n",
    "                'business', \n",
    "                'construction',\n",
    "                'health',\n",
    "                'environment', \n",
    "                'manufacturing', \n",
    "                'technology',\n",
    "                'program'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_suffixes = [\n",
    "                        'nb_le_f0_',\n",
    "                        'nb_ohe_f0_',\n",
    "                        'lrr_le_f0_',\n",
    "                        'lrr_ohe_f0_',\n",
    "                        'svm_le_f0_',\n",
    "                        'svm_ohe_f0_'\n",
    "                       ]\n",
    "\n",
    "binary_class_suffixes = [\n",
    "                        'nb_le_f1_',\n",
    "                        'nb_ohe_f1_',\n",
    "                        'lrr_le_f1_',\n",
    "                        'lrr_ohe_f1_',\n",
    "                        'svm_le_f1_',\n",
    "                        'svm_ohe_f1_',\n",
    "                        'tree_le_f1_',\n",
    "                        'tree_ohe_f1_'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block does all of the scoring, last one to update, only cell that needs running\n",
    "test_data_t7 = get_label_encoded_data('data/testing_data_t7.csv',model_name='t7',column_list=column_list,drop_not_happy='H',data_balance=False)[0]\n",
    "\n",
    "for experiment in experiment_model_names:\n",
    "    scoring_dictionary = {}\n",
    "    \n",
    "    for mclass in multi_class_suffixes:\n",
    "        temp_model_name = mclass+experiment\n",
    "        mclass_t3 = get_mclass_t3(temp_model_name)\n",
    "        mclass_RR = get_mclass_rr(temp_model_name)\n",
    "        mclass_accuracy = get_mclass_accuracy(temp_model_name)\n",
    "        mclass_loo = get_mclass_loo(temp_model_name)\n",
    "        mclass_5x = get_mclass_5x(temp_model_name)\n",
    "        scoring_dictionary[mclass+experiment] = {'t3':mclass_t3,'RR':mclass_RR,'accuracy':mclass_accuracy,'loo':mclass_loo,'5x':mclass_5x}\n",
    "   \n",
    "    for bclass in binary_class_suffixes:\n",
    "        temp_model_name = bclass+experiment\n",
    "        bclass_t3 = get_bclass_t3(temp_model_name)\n",
    "        bclass_RR = get_bclass_rr(temp_model_name)\n",
    "        bclass_accuracy = get_bclass_accuracy(temp_model_name)\n",
    "        bclass_loo = get_bclass_loo(temp_model_name)\n",
    "        bclass_5x = get_bclass_5x(temp_model_name)\n",
    "        scoring_dictionary[bclass+experiment] = {'t3':bclass_t3,'RR':bclass_RR,'accuracy':bclass_accuracy,'loo':bclass_loo,'5x':bclass_5x}\n",
    "        \n",
    "    save_scores(scoring_dictionary,experiment)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to be exported later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_probability_dict(p_df):\n",
    "    ordered_probabilties = sorted(p_df.values(),reverse=True)\n",
    "    ordered_programs = sorted(p_df, key=p_df.get,reverse=True)\n",
    "    return [p_df, ordered_probabilties, ordered_programs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_predict_proba(vector,temp_model_name):\n",
    "    return_probabilities_dict = {}\n",
    "    for program in list(INDEX_PROGRAM.keys()):\n",
    "        # Loading data used to build the model\n",
    "        model_name = temp_model_name+'_'+program\n",
    "        model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "        test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "        # Converting program labels to their appropriate binary label BIN_CLAS\n",
    "        temp_dictionary = INV_INDEX_PROGRAM.copy()\n",
    "        for key in INV_INDEX_PROGRAM.keys():\n",
    "            if str(key) != str(INDEX_PROGRAM[program]):\n",
    "                temp_dictionary[key] = -1\n",
    "            else:\n",
    "                temp_dictionary[key] = INDEX_PROGRAM[program]\n",
    "        test_data_t7_temp.program = test_data_t7_temp.program.map(temp_dictionary)\n",
    "\n",
    "        # Loading model files\n",
    "        pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "        index_dict = pickle.load(pkl_file)\n",
    "        new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "        pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "        model = pickle.load(pkl_file)\n",
    "\n",
    "        return_probabilities_dict[program] = model.predict_proba([vector])[0][1]\n",
    "\n",
    "    return (return_probabilities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(scoring_dictionary,experiment_model_name):\n",
    "    df = pd.DataFrame(scoring_dictionary) \n",
    "    df = df.T\n",
    "    df.to_csv(\"exported_model_files/scores/\"+experiment_model_name+\".csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mclass_accuracy(temp_model_name):\n",
    "    model_name = temp_model_name\n",
    "    model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "    test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "    # Loading model files\n",
    "    pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "    index_dict = pickle.load(pkl_file)\n",
    "    new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "    pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "    model = pickle.load(pkl_file)\n",
    "\n",
    "    # Getting average accuracy score\n",
    "    test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "    test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "    test_pred = []\n",
    "\n",
    "    for i in range(len(test_array)):\n",
    "        test_pred.append(model.predict([test_array[i]]))\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_pred,test_actual)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24047619047619048\n",
      "[0, 0.5, 1.0, 0, 0.5, 0, 0, 1.0, 0, 0, 0.5, 0, 1.0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0.5, 0, 0, 0, 0, 0.5, 0, 0.5, 0.5, 0, 0, 0, 1.0, 1.0, 0, 1.0, 0, 1.0, 0, 1.0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0.5, 0, 1.0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0.5, 0, 0.5, 0, 0.5, 1.0, 0.5, 1.0, 0, 0, 0, 0.5, 1.0, 0, 1.0, 0.5, 0, 0, 1.0, 1.0, 0, 1.0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.5, 1.0, 1.0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0.5, 0.5, 0, 1.0, 0, 0, 0, 0, 0.5, 0, 0.5, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'lrr_le_f0_d0_b0_c0_v0'\n",
    "model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "# Loading model files\n",
    "pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "index_dict = pickle.load(pkl_file)\n",
    "new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "model = pickle.load(pkl_file)\n",
    "\n",
    "# Getting average accuracy score\n",
    "test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "t3_scores = []\n",
    "for i in range(len(test_array)):\n",
    "    prediction = model.predict_proba([test_array[i]])\n",
    "    probs = sort_probability_dict(retrieve_prediction_labels(model,prediction))[2][:3]\n",
    "    n_probs = []\n",
    "    for prob in probs:\n",
    "        n_probs.append(INDEX_PROGRAM[prob])\n",
    "    try:\n",
    "        t3 = (1/n_probs.index(test_actual[i]))\n",
    "    except:\n",
    "        t3 = 0\n",
    "    t3_scores.append(t3)\n",
    "    \n",
    "return mean(t3_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mclass_t3(temp_model_name):\n",
    "    model_name = temp_model_name\n",
    "    model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "    test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "    # Loading model files\n",
    "    pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "    index_dict = pickle.load(pkl_file)\n",
    "    new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "    pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "    model = pickle.load(pkl_file)\n",
    "\n",
    "    # Getting average accuracy score\n",
    "    test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "    test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "    t3_scores = []\n",
    "    for i in range(len(test_array)):\n",
    "        prediction = model.predict_proba([test_array[i]])\n",
    "        probs = sort_probability_dict(retrieve_prediction_labels(model,prediction))[2][:3]\n",
    "        n_probs = []\n",
    "        for prob in probs:\n",
    "            n_probs.append(INDEX_PROGRAM[prob])\n",
    "        try:\n",
    "            t3 = (1/n_probs.index(test_actual[i]))\n",
    "        except:\n",
    "            t3 = 0\n",
    "        t3_scores.append(t3)\n",
    "\n",
    "    return mean(t3_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mclass_rr(temp_model_name):\n",
    "    model_name = temp_model_name\n",
    "    model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "    test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "    # Loading model files\n",
    "    pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "    index_dict = pickle.load(pkl_file)\n",
    "    new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "    pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "    model = pickle.load(pkl_file)\n",
    "\n",
    "    # Getting average accuracy score\n",
    "    test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "    test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "    rr_scores = []\n",
    "    for i in range(len(test_array)):\n",
    "        prediction = model.predict_proba([test_array[i]])\n",
    "        probs = sort_probability_dict(retrieve_prediction_labels(model,prediction))[2]\n",
    "        n_probs = []\n",
    "        for prob in probs:\n",
    "            n_probs.append(INDEX_PROGRAM[prob])\n",
    "        try:\n",
    "            rr = (1/n_probs.index(test_actual[i]))\n",
    "        except:\n",
    "            rr = 0\n",
    "        rr_scores.append(rr)\n",
    "\n",
    "    return mean(rr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mclass_loo(temp_model_name):\n",
    "    mean_loo = 0\n",
    "    return mean_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mclass_5x(temp_model_name):\n",
    "    mean_5x = 0\n",
    "    return mean_5x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bclass_accuracy(temp_model_name):\n",
    "    model_name = temp_model_name\n",
    "\n",
    "    model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "    test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "    # Getting average accuracy score\n",
    "    test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "    test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "    test_pred = []\n",
    "    for i in range(len(test_array)):\n",
    "        predicted = INDEX_PROGRAM[sort_probability_dict(binary_predict_proba(test_array[i],model_name))[2][0]]\n",
    "        test_pred.append(predicted)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_pred,test_actual)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bclass_t3(temp_model_name):\n",
    "    model_name = temp_model_name\n",
    "\n",
    "    model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "    test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "    # Getting average accuracy score\n",
    "    test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "    test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "    test_pred = []\n",
    "\n",
    "    for i in range(len(test_array)):\n",
    "            probs = sort_probability_dict(binary_predict_proba(test_array[i],model_name))[2][:3]\n",
    "            n_probs = []\n",
    "            for prob in probs:\n",
    "                n_probs.append(INDEX_PROGRAM[prob])\n",
    "            try:\n",
    "                t3 = (1/n_probs.index(test_actual[i]))\n",
    "            except:\n",
    "                t3 = 0\n",
    "            t3_scores.append(t3)\n",
    "\n",
    "    return mean(t3_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bclass_rr(temp_model_name):\n",
    "    model_name = temp_model_name\n",
    "\n",
    "    model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "    test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "\n",
    "    # Getting average accuracy score\n",
    "    test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "    test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "    test_pred = []\n",
    "\n",
    "    for i in range(len(test_array)):\n",
    "            probs = sort_probability_dict(binary_predict_proba(test_array[i],model_name))[2]\n",
    "            n_probs = []\n",
    "            for prob in probs:\n",
    "                n_probs.append(INDEX_PROGRAM[prob])\n",
    "            try:\n",
    "                rr = (1/n_probs.index(test_actual[i]))\n",
    "            except:\n",
    "                rr = 0\n",
    "            rr_scores.append(rr)\n",
    "\n",
    "    return mean(rr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bclass_loo(temp_model_name):\n",
    "    mean_loo = 0\n",
    "    return mean_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bclass_5x(temp_model_name):\n",
    "    mean_5x = 0\n",
    "    return mean_5x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b1_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inputs:\n",
    "- test_data\n",
    "- experiment_model_name\n",
    "'''\n",
    "model_name = 'nb_le_f1_d0_b0_c1_v0_nano'\n",
    "model_scores = {}\n",
    "model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "test_actual = np.array(test_data_t7_temp[\"program\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "index_dict = pickle.load(pkl_file)\n",
    "new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "model = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(test_array)):\n",
    "#     print(model.predict([test_array[i]]),test_actual[i])\n",
    "\n",
    "print(model.score(test_array,test_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_le_f0_d0_b0_c1_v0\n",
      "nb_ohe_f0_d0_b0_c1_v0\n",
      "lrr_le_f0_d0_b0_c1_v0\n",
      "lrr_ohe_f0_d0_b0_c1_v0\n",
      "svm_le_f0_d0_b0_c1_v0\n",
      "svm_ohe_f0_d0_b0_c1_v0\n",
      "nb_le_f1_d0_b0_c1_v0_mech\n",
      "0.8857142857142857\n",
      "nb_le_f1_d0_b0_c1_v0_bmed\n",
      "0.8285714285714286\n",
      "nb_le_f1_d0_b0_c1_v0_swe\n",
      "0.8285714285714286\n",
      "nb_le_f1_d0_b0_c1_v0_tron\n",
      "0.7857142857142857\n",
      "nb_le_f1_d0_b0_c1_v0_cive\n",
      "0.819047619047619\n",
      "nb_le_f1_d0_b0_c1_v0_chem\n",
      "0.861904761904762\n",
      "nb_le_f1_d0_b0_c1_v0_syde\n",
      "0.7857142857142857\n",
      "nb_le_f1_d0_b0_c1_v0_msci\n",
      "0.7904761904761904\n",
      "nb_le_f1_d0_b0_c1_v0_ce\n",
      "0.8857142857142857\n",
      "nb_le_f1_d0_b0_c1_v0_elec\n",
      "0.8333333333333334\n",
      "nb_le_f1_d0_b0_c1_v0_nano\n",
      "0.6714285714285714\n",
      "nb_le_f1_d0_b0_c1_v0_geo\n",
      "0.8285714285714286\n",
      "nb_le_f1_d0_b0_c1_v0_env\n",
      "0.8523809523809524\n",
      "nb_le_f1_d0_b0_c1_v0_arch-e\n",
      "0.8047619047619048\n",
      "nb_le_f1_d0_b0_c1_v0_arch\n",
      "0.8571428571428571\n",
      "nb_ohe_f1_d0_b0_c1_v0_mech\n",
      "0.8857142857142857\n",
      "nb_ohe_f1_d0_b0_c1_v0_bmed\n",
      "0.8285714285714286\n",
      "nb_ohe_f1_d0_b0_c1_v0_swe\n",
      "0.8285714285714286\n",
      "nb_ohe_f1_d0_b0_c1_v0_tron\n",
      "0.7523809523809524\n",
      "nb_ohe_f1_d0_b0_c1_v0_cive\n",
      "0.819047619047619\n",
      "nb_ohe_f1_d0_b0_c1_v0_chem\n",
      "0.8476190476190476\n",
      "nb_ohe_f1_d0_b0_c1_v0_syde\n",
      "0.7761904761904762\n",
      "nb_ohe_f1_d0_b0_c1_v0_msci\n",
      "0.7904761904761904\n",
      "nb_ohe_f1_d0_b0_c1_v0_ce\n",
      "0.8523809523809524\n",
      "nb_ohe_f1_d0_b0_c1_v0_elec\n",
      "0.8142857142857143\n",
      "nb_ohe_f1_d0_b0_c1_v0_nano\n",
      "0.6857142857142857\n",
      "nb_ohe_f1_d0_b0_c1_v0_geo\n",
      "0.8142857142857143\n",
      "nb_ohe_f1_d0_b0_c1_v0_env\n",
      "0.8666666666666667\n",
      "nb_ohe_f1_d0_b0_c1_v0_arch-e\n",
      "0.819047619047619\n",
      "nb_ohe_f1_d0_b0_c1_v0_arch\n",
      "0.8571428571428571\n",
      "lrr_le_f1_d0_b0_c1_v0_mech\n",
      "0.9333333333333333\n",
      "lrr_le_f1_d0_b0_c1_v0_bmed\n",
      "0.9571428571428572\n",
      "lrr_le_f1_d0_b0_c1_v0_swe\n",
      "0.8714285714285714\n",
      "lrr_le_f1_d0_b0_c1_v0_tron\n",
      "0.9238095238095239\n",
      "lrr_le_f1_d0_b0_c1_v0_cive\n",
      "0.8761904761904762\n",
      "lrr_le_f1_d0_b0_c1_v0_chem\n",
      "0.8857142857142857\n",
      "lrr_le_f1_d0_b0_c1_v0_syde\n",
      "0.8333333333333334\n",
      "lrr_le_f1_d0_b0_c1_v0_msci\n",
      "0.9285714285714286\n",
      "lrr_le_f1_d0_b0_c1_v0_ce\n",
      "0.9095238095238095\n",
      "lrr_le_f1_d0_b0_c1_v0_elec\n",
      "0.8428571428571429\n",
      "lrr_le_f1_d0_b0_c1_v0_nano\n",
      "0.8666666666666667\n",
      "lrr_le_f1_d0_b0_c1_v0_geo\n",
      "0.9285714285714286\n",
      "lrr_le_f1_d0_b0_c1_v0_env\n",
      "0.9428571428571428\n",
      "lrr_le_f1_d0_b0_c1_v0_arch-e\n",
      "0.8857142857142857\n",
      "lrr_le_f1_d0_b0_c1_v0_arch\n",
      "0.9571428571428572\n",
      "lrr_ohe_f1_d0_b0_c1_v0_mech\n",
      "0.9333333333333333\n",
      "lrr_ohe_f1_d0_b0_c1_v0_bmed\n",
      "0.9571428571428572\n",
      "lrr_ohe_f1_d0_b0_c1_v0_swe\n",
      "0.8714285714285714\n",
      "lrr_ohe_f1_d0_b0_c1_v0_tron\n",
      "0.9238095238095239\n",
      "lrr_ohe_f1_d0_b0_c1_v0_cive\n",
      "0.8761904761904762\n",
      "lrr_ohe_f1_d0_b0_c1_v0_chem\n",
      "0.8857142857142857\n",
      "lrr_ohe_f1_d0_b0_c1_v0_syde\n",
      "0.8333333333333334\n",
      "lrr_ohe_f1_d0_b0_c1_v0_msci\n",
      "0.9285714285714286\n",
      "lrr_ohe_f1_d0_b0_c1_v0_ce\n",
      "0.9095238095238095\n",
      "lrr_ohe_f1_d0_b0_c1_v0_elec\n",
      "0.8333333333333334\n",
      "lrr_ohe_f1_d0_b0_c1_v0_nano\n",
      "0.8333333333333334\n",
      "lrr_ohe_f1_d0_b0_c1_v0_geo\n",
      "0.8714285714285714\n",
      "lrr_ohe_f1_d0_b0_c1_v0_env\n",
      "0.9428571428571428\n",
      "lrr_ohe_f1_d0_b0_c1_v0_arch-e\n",
      "0.8857142857142857\n",
      "lrr_ohe_f1_d0_b0_c1_v0_arch\n",
      "0.9571428571428572\n",
      "svm_le_f1_d0_b0_c1_v0_mech\n",
      "0.9428571428571428\n",
      "svm_le_f1_d0_b0_c1_v0_bmed\n",
      "0.9571428571428572\n",
      "svm_le_f1_d0_b0_c1_v0_swe\n",
      "0.8714285714285714\n",
      "svm_le_f1_d0_b0_c1_v0_tron\n",
      "0.9238095238095239\n",
      "svm_le_f1_d0_b0_c1_v0_cive\n",
      "0.919047619047619\n",
      "svm_le_f1_d0_b0_c1_v0_chem\n",
      "0.861904761904762\n",
      "svm_le_f1_d0_b0_c1_v0_syde\n",
      "0.8333333333333334\n",
      "svm_le_f1_d0_b0_c1_v0_msci\n",
      "0.9285714285714286\n",
      "svm_le_f1_d0_b0_c1_v0_ce\n",
      "0.9095238095238095\n",
      "svm_le_f1_d0_b0_c1_v0_elec\n",
      "0.8428571428571429\n",
      "svm_le_f1_d0_b0_c1_v0_nano\n",
      "0.9\n",
      "svm_le_f1_d0_b0_c1_v0_geo\n",
      "0.3047619047619048\n",
      "svm_le_f1_d0_b0_c1_v0_env\n",
      "0.9428571428571428\n",
      "svm_le_f1_d0_b0_c1_v0_arch-e\n",
      "0.8857142857142857\n",
      "svm_le_f1_d0_b0_c1_v0_arch\n",
      "0.9571428571428572\n",
      "svm_ohe_f1_d0_b0_c1_v0_mech\n",
      "0.8857142857142857\n",
      "svm_ohe_f1_d0_b0_c1_v0_bmed\n",
      "0.9571428571428572\n",
      "svm_ohe_f1_d0_b0_c1_v0_swe\n",
      "0.8714285714285714\n",
      "svm_ohe_f1_d0_b0_c1_v0_tron\n",
      "0.9238095238095239\n",
      "svm_ohe_f1_d0_b0_c1_v0_cive\n",
      "0.8761904761904762\n",
      "svm_ohe_f1_d0_b0_c1_v0_chem\n",
      "0.861904761904762\n",
      "svm_ohe_f1_d0_b0_c1_v0_syde\n",
      "0.8333333333333334\n",
      "svm_ohe_f1_d0_b0_c1_v0_msci\n",
      "0.9285714285714286\n",
      "svm_ohe_f1_d0_b0_c1_v0_ce\n",
      "0.9095238095238095\n",
      "svm_ohe_f1_d0_b0_c1_v0_elec\n",
      "0.8333333333333334\n",
      "svm_ohe_f1_d0_b0_c1_v0_nano\n",
      "0.9\n",
      "svm_ohe_f1_d0_b0_c1_v0_geo\n",
      "0.8714285714285714\n",
      "svm_ohe_f1_d0_b0_c1_v0_env\n",
      "0.9095238095238095\n",
      "svm_ohe_f1_d0_b0_c1_v0_arch-e\n",
      "0.8857142857142857\n",
      "svm_ohe_f1_d0_b0_c1_v0_arch\n",
      "0.9571428571428572\n",
      "tree_le_f1_d0_b0_c1_v0_mech\n",
      "0.3047619047619048\n",
      "tree_le_f1_d0_b0_c1_v0_bmed\n",
      "0.9571428571428572\n",
      "tree_le_f1_d0_b0_c1_v0_swe\n",
      "0.8714285714285714\n",
      "tree_le_f1_d0_b0_c1_v0_tron\n",
      "0.9333333333333333\n",
      "tree_le_f1_d0_b0_c1_v0_cive\n",
      "0.2571428571428571\n",
      "tree_le_f1_d0_b0_c1_v0_chem\n",
      "0.9095238095238095\n",
      "tree_le_f1_d0_b0_c1_v0_syde\n",
      "0.8333333333333334\n",
      "tree_le_f1_d0_b0_c1_v0_msci\n",
      "0.8095238095238095\n",
      "tree_le_f1_d0_b0_c1_v0_ce\n",
      "0.9095238095238095\n",
      "tree_le_f1_d0_b0_c1_v0_elec\n",
      "0.8380952380952381\n",
      "tree_le_f1_d0_b0_c1_v0_nano\n",
      "0.15714285714285714\n",
      "tree_le_f1_d0_b0_c1_v0_geo\n",
      "0.8761904761904762\n",
      "tree_le_f1_d0_b0_c1_v0_env\n",
      "0.9428571428571428\n",
      "tree_le_f1_d0_b0_c1_v0_arch-e\n",
      "0.8857142857142857\n",
      "tree_le_f1_d0_b0_c1_v0_arch\n",
      "0.9523809523809523\n",
      "tree_ohe_f1_d0_b0_c1_v0_mech\n",
      "0.3047619047619048\n",
      "tree_ohe_f1_d0_b0_c1_v0_bmed\n",
      "0.9571428571428572\n",
      "tree_ohe_f1_d0_b0_c1_v0_swe\n",
      "0.8714285714285714\n",
      "tree_ohe_f1_d0_b0_c1_v0_tron\n",
      "0.9333333333333333\n",
      "tree_ohe_f1_d0_b0_c1_v0_cive\n",
      "0.919047619047619\n",
      "tree_ohe_f1_d0_b0_c1_v0_chem\n",
      "0.9333333333333333\n",
      "tree_ohe_f1_d0_b0_c1_v0_syde\n",
      "0.8333333333333334\n",
      "tree_ohe_f1_d0_b0_c1_v0_msci\n",
      "0.8095238095238095\n",
      "tree_ohe_f1_d0_b0_c1_v0_ce\n",
      "0.9095238095238095\n",
      "tree_ohe_f1_d0_b0_c1_v0_elec\n",
      "0.8380952380952381\n",
      "tree_ohe_f1_d0_b0_c1_v0_nano\n",
      "0.9\n",
      "tree_ohe_f1_d0_b0_c1_v0_geo\n",
      "0.861904761904762\n",
      "tree_ohe_f1_d0_b0_c1_v0_env\n",
      "0.9428571428571428\n",
      "tree_ohe_f1_d0_b0_c1_v0_arch-e\n",
      "0.8857142857142857\n",
      "tree_ohe_f1_d0_b0_c1_v0_arch\n",
      "0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "# This info should be under score_model_family()\n",
    "experiment_model_name = 'd0_b0_c1_v0'\n",
    "test_data_t7 = get_label_encoded_data('data/testing_data_t7.csv',model_name='t7',column_list=column_list,drop_not_happy='H',data_balance=False)[0]\n",
    "\n",
    "for experiment in experiment_model_names:\n",
    "    for mclass in multi_class_suffixes:\n",
    "        print(mclass+experiment)\n",
    "        x = 1\n",
    "    experiment = 'd0_b0_c1_v0'\n",
    "    for bclass in binary_class_suffixes:\n",
    "        for program in list(INDEX_PROGRAM.keys()):\n",
    "            # Loading data used to build the model\n",
    "            model_name = (bclass+experiment+'_'+program)\n",
    "            model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "            test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)]\n",
    "            \n",
    "            # Converting program labels to their appropriate binary label\n",
    "            temp_dictionary = INV_INDEX_PROGRAM.copy()\n",
    "            for key in INV_INDEX_PROGRAM.keys():\n",
    "                if str(key) != str(INDEX_PROGRAM[program]):\n",
    "                    temp_dictionary[key] = -1\n",
    "                else:\n",
    "                    temp_dictionary[key] = INDEX_PROGRAM[program]\n",
    "            \n",
    "            # Loading model files\n",
    "            pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "            index_dict = pickle.load(pkl_file)\n",
    "            new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "            pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "            model = pickle.load(pkl_file)\n",
    "            \n",
    "            # Getting average accuracy score\n",
    "            test_data_t7_temp.program = test_data_t7_temp.program.map(temp_dictionary)\n",
    "            test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "            test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "            test_pred = []\n",
    "            \n",
    "            for i in range(len(test_array)):\n",
    "                test_pred.append(model.predict([test_array[i]]))\n",
    "            \n",
    "            print(metrics.accuracy_score(test_pred,test_actual))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for program in list(INDEX_PROGRAM.keys()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
