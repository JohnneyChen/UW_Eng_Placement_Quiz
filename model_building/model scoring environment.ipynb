{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import metrics, tree, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold,cross_val_score,train_test_split,LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from statistics import mean\n",
    "\n",
    "from data_load import *\n",
    "from dictionaries import *\n",
    "from score_models import *\n",
    "\n",
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "time_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [\n",
    "                'problem_type',\n",
    "                'creative',\n",
    "                'outdoors',\n",
    "                'career',\n",
    "                'group_work',\n",
    "                'liked_courses',\n",
    "                'disliked_courses',\n",
    "                'programming',\n",
    "                'join_clubs',\n",
    "                'not_clubs',\n",
    "                'liked_projects',\n",
    "                'disliked_projects',\n",
    "                'tv_shows',\n",
    "                'alternate_degree',\n",
    "                'expensive_equipment',\n",
    "                'drawing',\n",
    "                'essay',\n",
    "                'architecture',\n",
    "                'automotive',\n",
    "                'business',\n",
    "                'construction',\n",
    "                'health',\n",
    "                'environment',\n",
    "                'manufacturing',\n",
    "                'technology',\n",
    "                'program'\n",
    "                ]\n",
    "\n",
    "ohe_main =  [\n",
    "            'problem_type',\n",
    "            'creative',\n",
    "            'outdoors',\n",
    "            'career',\n",
    "            'group_work',\n",
    "            'liked_courses',\n",
    "            'disliked_courses',\n",
    "            'programming',\n",
    "            'join_clubs',\n",
    "            'not_clubs',\n",
    "            'liked_projects',\n",
    "            'disliked_projects',\n",
    "            'tv_shows',\n",
    "            'alternate_degree',\n",
    "            'expensive_equipment',\n",
    "            'drawing',\n",
    "            'essay'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the model experiment names and all of the used suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep the suffixes and the experiment model names updated to view all scores\n",
    "\n",
    "### It takes about 10 mins to score each experiment model name\n",
    "\n",
    "### If a new type of encoding is being tried, changes need to be made to the scoring methods. Call Ayser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_model_names = [\n",
    "#                         'd0_b0_c0_v0',\n",
    "#                         'd0_b0_c1_v0',\n",
    "#                         'd0_b0_c2_v0',\n",
    "#                         'd0_b0_c3_v0',\n",
    "#                         'd0_b1_c3_v0',\n",
    "#                         'd0_b1_c0_v0',\n",
    "#                         'd0_b1_c1_v0',\n",
    "#                         'd0_b1_c2_v0',\n",
    "#                         'd0_b0_c4_v0',\n",
    "#                         'd0_b1_c4_v0',\n",
    "#                         'd0_b0_c5_v0',\n",
    "#                         'd0_b0_c29_v0',\n",
    "#                         'd0_b1_c29_v0'\n",
    "                        'd0_b1_c36_v0',\n",
    "                        'd0_b0_c36_v0'\n",
    "                        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_suffixes = [\n",
    "                        'nb_le_f0_',\n",
    "                        'nb_ohe_f0_',\n",
    "                        'lrr_le_f0_',\n",
    "                        'lrr_ohe_f0_',\n",
    "                        'svm_le_f0_',\n",
    "                        'svm_ohe_f0_'\n",
    "                       ]\n",
    "\n",
    "binary_class_suffixes = [\n",
    "                        'nb_le_f1_',\n",
    "                        'nb_ohe_f1_',\n",
    "                        'lrr_le_f1_',\n",
    "                        'lrr_ohe_f1_',\n",
    "                        'svm_le_f1_',\n",
    "                        'svm_ohe_f1_',\n",
    "                        'tree_le_f1_',\n",
    "                        'tree_ohe_f1_'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_le_f0_d0_b1_c36_v0 scored...\n",
      "nb_ohe_f0_d0_b1_c36_v0 scored...\n",
      "lrr_le_f0_d0_b1_c36_v0 scored...\n",
      "lrr_ohe_f0_d0_b1_c36_v0 scored...\n",
      "svm_le_f0_d0_b1_c36_v0 scored...\n",
      "svm_ohe_f0_d0_b1_c36_v0 scored...\n",
      "saving d0_b1_c36_v0 scores..\n",
      "nb_le_f0_d0_b0_c36_v0 scored...\n",
      "nb_ohe_f0_d0_b0_c36_v0 scored...\n",
      "lrr_le_f0_d0_b0_c36_v0 scored...\n",
      "lrr_ohe_f0_d0_b0_c36_v0 scored...\n",
      "svm_le_f0_d0_b0_c36_v0 scored...\n",
      "svm_ohe_f0_d0_b0_c36_v0 scored...\n",
      "saving d0_b0_c36_v0 scores..\n",
      "scoring complete.\n"
     ]
    }
   ],
   "source": [
    "# This block does all of the scoring, last one to update, only cell that needs running\n",
    "for experiment in experiment_model_names:\n",
    "    scoring_dictionary = {}\n",
    "    \n",
    "    for mclass in multi_class_suffixes:\n",
    "        temp_model_name = mclass+experiment\n",
    "        model_name = temp_model_name\n",
    "        \n",
    "        model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "        ohe = ohe_main\n",
    "        # Loading test data\n",
    "        if 'le' in model_name:\n",
    "            test_data_t7 = get_label_encoded_data('data/t7.csv',model_name='t7',column_list=column_list,drop_not_happy='H',data_balance=False)[0]\n",
    "        elif 'ohe' in model_name:\n",
    "            test_data_t7 = get_merged_encoded_data(directory = 'data/t7.csv',model_name ='t7',one_hot_encode=ohe,column_list = column_list,drop_not_happy='H',data_balance=False)\n",
    "       \n",
    "        test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)].head(210)\n",
    "\n",
    "        # Loading model files\n",
    "        pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "        index_dict = pickle.load(pkl_file)\n",
    "        new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "        pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "        model = pickle.load(pkl_file)\n",
    "\n",
    "        # Preparing Loading data\n",
    "        test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "        test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "        \n",
    "        mclass_t3 = get_mclass_t3(temp_model_name,model,test_array,test_actual)\n",
    "        mclass_RR = get_mclass_rr(temp_model_name,model,test_array,test_actual)\n",
    "        mclass_accuracy = get_mclass_accuracy(temp_model_name,model,test_array,test_actual)\n",
    "        mclass_reassignment = get_mclass_reassignment(temp_model_name,model)\n",
    "        \n",
    "        scoring_dictionary[mclass+experiment] = {'t3':mclass_t3,'RR':mclass_RR,'accuracy':mclass_accuracy,'reassignment':mclass_reassignment}\n",
    "        print(str(temp_model_name)+\" scored...\")\n",
    "        \n",
    "#     for bclass in binary_class_suffixes:\n",
    "#         temp_model_name = bclass+experiment\n",
    "#         model_name = temp_model_name\n",
    "        \n",
    "#         model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "#         ohe = ohe_main\n",
    "\n",
    "#         # Loading test data\n",
    "#         if 'le' in model_name:\n",
    "#             test_data_t7 = get_label_encoded_data('data/t7.csv',model_name='t7',column_list=column_list,drop_not_happy='H',data_balance=False)[0]\n",
    "#         elif 'ohe' in model_name:\n",
    "#             test_data_t7 = get_merged_encoded_data(directory = 'data/t7.csv',model_name ='t7',one_hot_encode=ohe,column_list = column_list,drop_not_happy='H',data_balance=False)\n",
    "            \n",
    "\n",
    "#         test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)].head(210)\n",
    " \n",
    "#         # Getting average accuracy score\n",
    "#         test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "#         test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "        \n",
    "#         bclass_t3 = get_bclass_t3(test_array,model_name,test_actual,test_data_t7)\n",
    "#         bclass_RR = get_bclass_rr(test_array,model_name,test_actual,test_data_t7)\n",
    "#         bclass_accuracy = get_bclass_accuracy(test_array,model_name,test_actual,test_data_t7)\n",
    "#         bclass_reassignment = get_bclass_reassignment(test_array,model_name,test_data_t7)\n",
    "#         scoring_dictionary[bclass+experiment] = {'t3':bclass_t3,'RR':bclass_RR,'accuracy':bclass_accuracy,'reassignment':bclass_reassignment}\n",
    "#         print(str(temp_model_name)+\" scored...\")\n",
    "        \n",
    "    print(\"saving \"+str(experiment)+ \" scores..\")\n",
    "    save_scores(scoring_dictionary,experiment)\n",
    "    b = datetime.datetime.now()\n",
    "    c = b - a\n",
    "    a = datetime.datetime.now()\n",
    "    time_list.append(c)\n",
    "    \n",
    "    \n",
    "print(\"scoring complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 seconds or 0.15 mins\n",
      "2 seconds or 0.03333333333333333 mins\n"
     ]
    }
   ],
   "source": [
    "for c in time_list:\n",
    "    print(str(c.seconds)+\" seconds or \"+str(c.seconds/60)+\" mins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
