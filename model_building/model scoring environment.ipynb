{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import metrics, tree, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold,cross_val_score,train_test_split,LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from statistics import mean\n",
    "\n",
    "from data_load import *\n",
    "from dictionaries import *\n",
    "from score_models import *\n",
    "\n",
    "import datetime\n",
    "# a = datetime.datetime.now()\n",
    "# time_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [\n",
    "                'problem_type',\n",
    "                'creative',\n",
    "                'outdoors',\n",
    "                'career',\n",
    "                'group_work',\n",
    "                'liked_courses',\n",
    "                'disliked_courses',\n",
    "                'programming',\n",
    "                'join_clubs',\n",
    "                'not_clubs',\n",
    "                'liked_projects',\n",
    "                'disliked_projects',\n",
    "                'tv_shows',\n",
    "                'alternate_degree',\n",
    "                'expensive_equipment',\n",
    "                'drawing',\n",
    "                'essay',\n",
    "                'architecture',\n",
    "                'automotive',\n",
    "                'business',\n",
    "                'construction',\n",
    "                'health',\n",
    "                'environment',\n",
    "                'manufacturing',\n",
    "                'technology',\n",
    "                'program'\n",
    "                ]\n",
    "\n",
    "ohe_main =  [\n",
    "            'problem_type',\n",
    "            'creative',\n",
    "            'outdoors',\n",
    "            'career',\n",
    "            'group_work',\n",
    "            'liked_courses',\n",
    "            'disliked_courses',\n",
    "            'programming',\n",
    "            'join_clubs',\n",
    "            'not_clubs',\n",
    "            'liked_projects',\n",
    "            'disliked_projects',\n",
    "            'tv_shows',\n",
    "            'alternate_degree',\n",
    "            'expensive_equipment',\n",
    "            'drawing',\n",
    "            'essay'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the model experiment names and all of the used suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep the suffixes and the experiment model names updated to view all scores\n",
    "\n",
    "### It takes about 10 mins to score each experiment model name\n",
    "\n",
    "### If a new type of encoding is being tried, changes need to be made to the scoring methods. Call Ayser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_model_names = [\n",
    "#                         'd0_b0_c0_v0',\n",
    "#                         'd0_b0_c1_v0',\n",
    "#                         'd0_b0_c2_v0',\n",
    "#                         'd0_b0_c3_v0',\n",
    "#                         'd0_b1_c3_v0',\n",
    "#                         'd0_b1_c0_v0',\n",
    "#                         'd0_b1_c1_v0',\n",
    "#                         'd0_b1_c2_v0',\n",
    "#                         'd0_b0_c4_v0',\n",
    "#                         'd0_b1_c4_v0',\n",
    "#                         'd0_b0_c5_v0',\n",
    "#                         'd0_b0_c29_v0',\n",
    "#                         'd0_b1_c29_v0'\n",
    "                        'd0_b1_c36_v0',\n",
    "                        'd0_b0_c36_v0'\n",
    "                        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_suffixes = [\n",
    "                        'nb_le_f0_',\n",
    "                        'nb_ohe_f0_',\n",
    "                        'lrr_le_f0_',\n",
    "                        'lrr_ohe_f0_',\n",
    "                        'svm_le_f0_',\n",
    "                        'svm_ohe_f0_'\n",
    "                       ]\n",
    "\n",
    "binary_class_suffixes = [\n",
    "                        'nb_le_f1_',\n",
    "                        'nb_ohe_f1_',\n",
    "                        'lrr_le_f1_',\n",
    "                        'lrr_ohe_f1_',\n",
    "                        'svm_le_f1_',\n",
    "                        'svm_ohe_f1_',\n",
    "                        'tree_le_f1_',\n",
    "                        'tree_ohe_f1_'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_le_f0_d0_b1_c36_v0 scored...\n",
      "nb_ohe_f0_d0_b1_c36_v0 scored...\n",
      "lrr_le_f0_d0_b1_c36_v0 scored...\n",
      "lrr_ohe_f0_d0_b1_c36_v0 scored...\n",
      "svm_le_f0_d0_b1_c36_v0 scored...\n",
      "svm_ohe_f0_d0_b1_c36_v0 scored...\n",
      "saving d0_b1_c36_v0 scores..\n",
      "nb_le_f0_d0_b0_c36_v0 scored...\n",
      "nb_ohe_f0_d0_b0_c36_v0 scored...\n",
      "lrr_le_f0_d0_b0_c36_v0 scored...\n",
      "lrr_ohe_f0_d0_b0_c36_v0 scored...\n",
      "svm_le_f0_d0_b0_c36_v0 scored...\n",
      "svm_ohe_f0_d0_b0_c36_v0 scored...\n",
      "saving d0_b0_c36_v0 scores..\n",
      "scoring complete.\n"
     ]
    }
   ],
   "source": [
    "# This block does all of the scoring, last one to update, only cell that needs running\n",
    "for experiment in experiment_model_names:\n",
    "    scoring_dictionary = {}\n",
    "    \n",
    "    for mclass in multi_class_suffixes:\n",
    "        temp_model_name = mclass+experiment\n",
    "        model_name = temp_model_name\n",
    "        \n",
    "        model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "        ohe = ohe_main\n",
    "        # Loading test data\n",
    "        if 'le' in model_name:\n",
    "            test_data_t7 = get_label_encoded_data('data/t7.csv',model_name='t7',column_list=column_list,drop_not_happy='H',data_balance=False)[0]\n",
    "        elif 'ohe' in model_name:\n",
    "            test_data_t7 = get_merged_encoded_data(directory = 'data/t7.csv',model_name ='t7',one_hot_encode=ohe,column_list = column_list,drop_not_happy='H',data_balance=False)\n",
    "       \n",
    "        test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)].head(210)\n",
    "\n",
    "        # Loading model files\n",
    "        pkl_file = open('exported_model_files/metadata/'+model_name+'_cat', 'rb')\n",
    "        index_dict = pickle.load(pkl_file)\n",
    "        new_vector = np.zeros(len(index_dict))\n",
    "\n",
    "        pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "        model = pickle.load(pkl_file)\n",
    "\n",
    "        # Preparing Loading data\n",
    "        test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "        test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "        \n",
    "        mclass_t3 = get_mclass_t3(temp_model_name,model,test_array,test_actual)\n",
    "        mclass_RR = get_mclass_rr(temp_model_name,model,test_array,test_actual)\n",
    "        mclass_accuracy = get_mclass_accuracy(temp_model_name,model,test_array,test_actual)\n",
    "        mclass_reassignment = get_mclass_reassignment(temp_model_name,model)\n",
    "        \n",
    "        scoring_dictionary[mclass+experiment] = {'t3':mclass_t3,'RR':mclass_RR,'accuracy':mclass_accuracy,'reassignment':mclass_reassignment}\n",
    "        print(str(temp_model_name)+\" scored...\")\n",
    "        \n",
    "#     for bclass in binary_class_suffixes:\n",
    "#         temp_model_name = bclass+experiment\n",
    "#         model_name = temp_model_name\n",
    "        \n",
    "#         model_data = pd.read_csv('exported_model_files/dataframes/'+model_name+'.csv',dtype=str)\n",
    "#         ohe = ohe_main\n",
    "\n",
    "#         # Loading test data\n",
    "#         if 'le' in model_name:\n",
    "#             test_data_t7 = get_label_encoded_data('data/t7.csv',model_name='t7',column_list=column_list,drop_not_happy='H',data_balance=False)[0]\n",
    "#         elif 'ohe' in model_name:\n",
    "#             test_data_t7 = get_merged_encoded_data(directory = 'data/t7.csv',model_name ='t7',one_hot_encode=ohe,column_list = column_list,drop_not_happy='H',data_balance=False)\n",
    "            \n",
    "\n",
    "#         test_data_t7_temp = test_data_t7.copy()[list(model_data.columns)].head(210)\n",
    " \n",
    "#         # Getting average accuracy score\n",
    "#         test_array = np.array(test_data_t7_temp.drop(axis=1,columns=[\"program\"]))\n",
    "#         test_actual = np.array(test_data_t7_temp[\"program\"])\n",
    "        \n",
    "#         bclass_t3 = get_bclass_t3(test_array,model_name,test_actual,test_data_t7)\n",
    "#         bclass_RR = get_bclass_rr(test_array,model_name,test_actual,test_data_t7)\n",
    "#         bclass_accuracy = get_bclass_accuracy(test_array,model_name,test_actual,test_data_t7)\n",
    "#         bclass_reassignment = get_bclass_reassignment(test_array,model_name,test_data_t7)\n",
    "#         scoring_dictionary[bclass+experiment] = {'t3':bclass_t3,'RR':bclass_RR,'accuracy':bclass_accuracy,'reassignment':bclass_reassignment}\n",
    "#         print(str(temp_model_name)+\" scored...\")\n",
    "        \n",
    "    print(\"saving \"+str(experiment)+ \" scores..\")\n",
    "    save_scores(scoring_dictionary,experiment)\n",
    "    b = datetime.datetime.now()\n",
    "    c = b - a\n",
    "    a = datetime.datetime.now()\n",
    "    time_list.append(c)\n",
    "    \n",
    "    \n",
    "print(\"scoring complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 seconds or 0.05 mins\n",
      "3 seconds or 0.05 mins\n"
     ]
    }
   ],
   "source": [
    "for c in time_list:\n",
    "    print(str(c.seconds)+\" seconds or \"+str(c.seconds/60)+\" mins\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "new_vector = np.zeros(21)\n",
    "new_vector[0] =  '1'\n",
    "new_vector[1] =  '1'\n",
    "new_vector[2] =  '1'\n",
    "new_vector[3] =  '1'\n",
    "new_vector[4] =  '1'\n",
    "new_vector[5] =  '1'\n",
    "# new_vector[7] =  0 #programming[post_dict['programming'][0]]\n",
    "new_vector[6] =  '1'\n",
    "new_vector[7] =  '1'\n",
    "new_vector[8] = '1'\n",
    "new_vector[9] = '1'\n",
    "# new_vector[12] = 0 #tv_shows[post_dict['tv_shows'][0]]\n",
    "new_vector[10] = '1'\n",
    "# new_vector[14] = 0 #expensive_equipment[post_dict['expensive_equipment'][0]]\n",
    "new_vector[11] = '1'\n",
    "new_vector[12] = '1'\n",
    "new_vector[13] = '1'\n",
    "new_vector[14] = '1'\n",
    "new_vector[15] = '1'\n",
    "new_vector[16] = '1'\n",
    "new_vector[17] = '1'\n",
    "new_vector[18] = '1'\n",
    "new_vector[19] = '1'\n",
    "new_vector[20] = '1'\n",
    "\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "new_vector = list(new_vector)\n",
    "for i in range(len(new_vector)):\n",
    "    new_vector[i]  = str(int(new_vector[i]))\n",
    "\n",
    "columns = [\n",
    "            'creative',\n",
    "            'outdoors',\n",
    "            'career',\n",
    "            'group_work',\n",
    "            'liked_courses',\n",
    "            'disliked_courses',\n",
    "            'join_clubs',\n",
    "            'not_clubs',\n",
    "            'liked_projects',\n",
    "            'disliked_projects',\n",
    "            'alternate_degree',\n",
    "            'drawing',\n",
    "            'essay',\n",
    "            'architecture',\n",
    "            'automotive',\n",
    "            'business',\n",
    "            'construction',\n",
    "            'health',\n",
    "            'environment',\n",
    "            'manufacturing',\n",
    "            'technology'\n",
    "]\n",
    "t7 = get_label_encoded_data('data/t7.csv',model_name='t7',column_list=columns,drop_not_happy='H',data_balance=False)[0]\n",
    "\n",
    "# t7.append(df)\n",
    "print(len(columns))\n",
    "data_to_append = {}\n",
    "for i in range(len(columns)):\n",
    "    data_to_append[t7.columns[i]] = int(new_vector[i])\n",
    "t7 = t7.append(data_to_append, ignore_index = True)\n",
    "t7 = pd.get_dummies(t7,columns=columns)\n",
    "new_vector = np.array(t7[len(t7)-1:len(t7)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nb_ohe_f0_d0_b0_c36_v0'\n",
    "pkl_file = open('exported_model_files/models/'+model_name+'.pkl', 'rb')\n",
    "model = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,107) and (99,15) not aligned: 107 (dim 1) != 99 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-daa8ed1c046a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_log_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_log_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# normalize by P(x) = P(f_1, ..., f_n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mlog_prob_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    732\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,107) and (99,15) not aligned: 107 (dim 1) != 99 (dim 0)"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_proba(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
